apiVersion: vllm.ai/v1
kind: VLLM
metadata:
  name: llm-runtime-llama
  namespace: default
spec:
  namespace: default
  runtimeName: llama-3.1-8b
  model: "llama-3.1-8b"
  action: start
  replicas: 1
  args: 
    - "--disable-log-requests"
    - "--max-model-len=4096"
    - "--dtype=bfloat16"
    - "--max-num-seqs=32"
    - "--tensor-parallel-size=1"
    - "--gpu-memory-utilization=0.8"
  storageUri: "file:///usr/local/models/llama-3.1-8b"
  
  vllmConfig:
    port: 8000
    v1: true
    env:
      - name: HF_HOME
        value: "/data"
  deploymentConfig:
    resources:
      limits:
        nvidia.com/gpu: "1"
      requests:
        cpu: "10"
        memory: "32Gi"
    image:
      registry: "docker.io"
      name: "lmcache/vllm-openai:2025-05-27-v1"
      pullPolicy: "IfNotPresent"
    deploymentStrategy: "Recreate"
    volumeMounts:
      - name: model-storage
        mountPath: "/usr/local/models"
    volumes:
      - name: model-storage
        persistentVolumeClaim:
          claimName: llm-models-pvc