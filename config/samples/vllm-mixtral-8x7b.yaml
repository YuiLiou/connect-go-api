apiVersion: vllm.ai/v1
kind: VLLMRuntime
metadata:
  name: llm-runtime-mistral
  namespace: default
spec:
  action: start
  model:
    modelURL: "file:///usr/local/models/Mixtral-8x7B-Instruct-v0.1"  # 本地路徑
    enableLoRA: false
    enableTool: false
    maxModelLen: 4096
    dtype: "bfloat16"
    maxNumSeqs: 32
  vllmConfig:
    enableChunkedPrefill: false
    enablePrefixCaching: false
    tensorParallelSize: 1
    gpuMemoryUtilization: "0.8"
    extraArgs: ["--disable-log-requests"]
    v1: true
    port: 8000
    env:
      - name: HF_HOME
        value: "/data"  # 可選：快取目錄
  lmCacheConfig:
    enabled: true
    cpuOffloadingBufferSize: "15"
    diskOffloadingBufferSize: "0"
    remoteUrl: "lm://cacheserver-sample.default.svc.cluster.local:80"
    remoteSerde: "naive"
  deploymentConfig:
    resources:
      limits:
        nvidia.com/gpu: "1"
      requests:
        cpu: "10"
        memory: "32Gi"
    image:
      registry: "docker.io"
      name: "lmcache/vllm-openai:2025-05-27-v1"
      pullPolicy: "IfNotPresent"
    replicas: 1
    deploymentStrategy: "Recreate"
    volumeMounts:  # 掛載 PVC
      - name: model-storage
        mountPath: "/usr/local/models"
    volumes:  # 定義 Volume
      - name: model-storage
        persistentVolumeClaim:
          claimName: llm-models-pvc
